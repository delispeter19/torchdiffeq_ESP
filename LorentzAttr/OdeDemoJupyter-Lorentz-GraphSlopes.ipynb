{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import torch\n",
    "import torch.nn as nn        #for all Neural Network stuff\n",
    "import torch.optim as optim  #for all optimizer stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method argument not needed??\n",
    "data_size = 2000 #number of points in linspace\n",
    "batch_time = 20 #number of points in the interval after a random starting point\n",
    "batch_size = 500 #number of intervals\n",
    "niters = 2000 #number of iterations\n",
    "test_freq = 1\n",
    "viz = True\n",
    "gpu = 1\n",
    "adjoint = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdiffeq import odeint_adjoint as odeint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:' + str(gpu) if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create linspace like usual\n",
    "t = torch.linspace(0.0, 25.0, data_size)\n",
    "\n",
    "sigma = 10.0\n",
    "ro = 28.0\n",
    "beta = 8.0/3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random():\n",
    "    rand = 0\n",
    "    while abs(rand) < 1 or abs(rand) > 3:\n",
    "        rand = np.random.normal(0, 1, 1)[0]\n",
    "    return rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSlopesTorch(x,function):\n",
    "    dim =  x.shape[0]**3\n",
    "    slopeList = torch.zeros((dim,3))\n",
    "    i = 0\n",
    "    for m in x:\n",
    "        for l in x:\n",
    "            for k in x:\n",
    "                #print(slope([],[m,l,k]))\n",
    "                slopeList[i] = function.forward([],torch.tensor([[m,l,k]]))\n",
    "                i += 1\n",
    "    return slopeList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lambda is a nn.Module like Linear, Conv2D etc,,\n",
    "class Lambda(nn.Module):\n",
    "    def forward(self,t,u):\n",
    "        x = u[0,0]\n",
    "        y = u[0,1]\n",
    "        z = u[0,2]\n",
    "\n",
    "        DE = np.zeros_like(u[0])\n",
    "\n",
    "        DE[0] = sigma*(y-x)\n",
    "        DE[1] = x*(ro-z)-y\n",
    "        DE[2] = x*y - beta*z\n",
    "\n",
    "        return torch.tensor(DE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch():\n",
    "    s = torch.from_numpy(np.random.choice(np.arange(data_size-batch_time, dtype=np.int64), batch_size, replace=False))\n",
    "#s is a torch tensor from a numpy array. the array is a random choice array, a rand number from 0 to ds-bt is chosen bs times.\n",
    "#s has batch_size elements and each was equally likely to be chosen\n",
    "#why ds-bt? why not just data_size as a rance since we are chosing from the t linspace with ds many points?\n",
    "#it does ds-bt so that when it chooses the next bt number of points we will never fall out of the index range if\n",
    "#    ds=1000 and bt=10 then then if i start at 990, I will end up with 990-1000 and not over.\n",
    "    batch_y0 = true_y[s] #(M, D) <- idk what this means\n",
    "#b_y0 is made of the randomly selected data points (dep vals) from true_y solution\n",
    "    batch_t = t[:batch_time] #(T)\n",
    "#b_t is the values rank 0 to b_time from your linspace t; the linspace values being looked at\n",
    "    batch_y = torch.stack([true_y[s+i] for i in range(batch_time)],dim=0)  # (T, M, D)\n",
    "    return batch_y0, batch_t, batch_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makedirs(dirname):\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "makedirs('png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawDFT(ySpace, length, function):\n",
    "    for l in ySpace:\n",
    "        for k in ySpace:\n",
    "            y = torch.tensor([[l,k]])\n",
    "            t = torch.tensor([[]])\n",
    "            #direction i and j + Magnitude of <i,j>\n",
    "            i = function.forward(t,y)[:,0][0].numpy()\n",
    "            j = function.forward(t,y)[:,1][0].numpy()\n",
    "            magnitude = np.sqrt(i**2+j**2)\n",
    "            \n",
    "            #normalize and scale by h\n",
    "            i = h*i/magnitude\n",
    "            j = h*j/magnitude\n",
    "            \n",
    "            current_domain = np.linspace(l-(i/2), l+(i/2), 2) #the end points of the small intervals\n",
    "            current_range = np.linspace(k-(j/2), k+(j/2), 2)\n",
    "\n",
    "            plt.plot(current_domain, current_range, lw=0.2, color='b')\n",
    "            plt.arrow(current_domain[0],current_range[0], i, j, color='b', lw=0.5, length_includes_head=True, head_length=0.1, head_width=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(y, color):\n",
    "    \n",
    "    #Draw True Y\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes(projection='3d')\n",
    "    \n",
    "    ax.plot3D(y.numpy()[:,0,0],y.numpy()[:,0,1],y.numpy()[:,0,2],color)\n",
    "    plt.xlim(-30,30)\n",
    "    plt.ylim(-30,30)\n",
    "    ax.set_zlim(10,45)\n",
    "    plt.savefig('png/{:03d}'.format(itr))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeSl(Sl,color):\n",
    "    \n",
    "    slxT = torch.zeros(8000)\n",
    "    slyT = torch.zeros(8000)\n",
    "    slzT = torch.zeros(8000)\n",
    "    \n",
    "    for i in range(0,8000):\n",
    "        slxT[i] = Sl[i,0]\n",
    "        slyT[i] = Sl[i,1]\n",
    "        slzT[i] = Sl[i,2]\n",
    "    \n",
    "    #Draw \n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes(projection='3d')\n",
    "\n",
    "    ax.plot3D(slyT.numpy(),slxT.numpy(),slzT.numpy(),color)\n",
    "\n",
    "    #drawDFNLorentz(Y.numpy(),h)\n",
    "    plt.xlim(-2500,2500)\n",
    "    plt.ylim(-2500,2500)\n",
    "    ax.set_zlim(-2500,2500)\n",
    "    plt.savefig('png/{:03d}'.format(itr))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODEFunc(nn.Module): #create an ODE block as our neural network. usually multiple layers/modules\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ODEFunc, self).__init__()\n",
    "#self.net is the neural network layers one after the other L -> T -> L        \n",
    "        self.net = nn.Sequential(    \n",
    "            nn.Linear(3,50),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(50,100),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(100,50),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(50,3),\n",
    "        )\n",
    "        \n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m,nn.Linear): #checks in the module m in self.net is an instance of the nn.Linear class\n",
    "                nn.init.normal_(m.weight, mean=0, std=200.0) #for the Linear modules make all the weights randn with std 0.1\n",
    "                nn.init.constant_(m.bias, val=0) #for the Linear modules make all the bias vector values 0\n",
    "    \n",
    "    def forward(self, t, y):\n",
    "        return self.net(y) #essentially Lambda.forward(args) without the true_A >>>which is TBD<<<"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningAverageMeter(object): #Computes and stores the average and current value\n",
    "    def __init__(self, momentum=0.99): #new object of this class will have mom of 0.99 and a reset val and avg\n",
    "        self.momentum = momentum\n",
    "        self.reset()\n",
    "    def reset(self):                   #reset function called to set val and avg to None and 0 respectively\n",
    "        self.val = None\n",
    "        self.avg = 0\n",
    "    def update(self, val):             #update is called externally to update the avg based on val\n",
    "        if self.val is None:\n",
    "            self.avg = val\n",
    "        else:\n",
    "            self.avg = (self.avg * self.momentum) + (val * (1 - self.momentum)) #reminds me of output error at L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trueSl\n",
      "predSl\n",
      "loss\n",
      "1\n",
      "2\n",
      "3\n",
      "BPdone\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    #ii = 0\n",
    "    X = torch.linspace(-45,45,45)\n",
    "    \n",
    "    func = ODEFunc()\n",
    "    optimizer = optim.RMSprop(func.parameters())          #this is an optimizer designed to update parameters\n",
    "    end = time.time()                                     #usually people use SGD but this is a mini-batch approach\n",
    "                                                          #http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf\n",
    "    #RunningAvgMeter Objects\n",
    "    time_meter = RunningAverageMeter(0.97)\n",
    "    loss_meter = RunningAverageMeter(0.97)\n",
    "    \n",
    "    #TRAINING\n",
    "    for itr in range(1, niters+1):\n",
    "        #initial conditions; requires 2D tensor\n",
    "        true_y0 = torch.tensor([[random(),random(),random()]])\n",
    "        \n",
    "        with torch.no_grad(): #no_grad disables gradient finding when true_y is defined\n",
    "            #print(torch.mm(true_y0**3,true_A))\n",
    "            true_y = odeint(Lambda(),true_y0, t, method='adams') \n",
    "            #true_y is the solved ODE defined in Lambda\n",
    "            #odeint only takes a function from nn.Module >> implications? see solver comparision Jupyter\n",
    "            #it uses Lambda to provide the function to go solve the IVP given true_y0 with the ODE solver on t\n",
    "            trueSl = getSlopesTorch(X.numpy(),Lambda())\n",
    "        print(\"trueSl\")\n",
    "        \n",
    "        optimizer.zero_grad() #set gradients to zero/ not computed\n",
    "        \n",
    "        #batch_y0, batch_t, batch_y = get_batch()\n",
    "        #feeds odeint the ODEBlock, a set of y values based on random t values and the first ten values of t\n",
    "        #pred_y = odeint(func, batch_y0, batch_t)\n",
    "        #deviates from usual Loss of mean squared difference but sum in both cases is positive\n",
    "        #print(pred_y.detach(),batch_y)\n",
    "        #---------------------------------------\n",
    "        predSl = getSlopesTorch(X.numpy(),func)\n",
    "        print(\"predSl\")\n",
    "        \n",
    "        #loss = torch.mean(torch.abs(pred_y-batch_y)**2) #ask Ivan about predY-bY, also this returns a number\n",
    "        #--------------------------------------------------\n",
    "        loss = torch.mean(torch.abs(predSl-trueSl)**2)\n",
    "        print(\"loss\")\n",
    "        \n",
    "        loss.backward() #backpropogation\n",
    "        print(\"1\")\n",
    "        optimizer.step() #gradient descent and update parameters\n",
    "        print(\"2\")\n",
    "        #Update time and loss meters\n",
    "        time_meter.update(time.time()-end) #takes in time difference to establish velocity see RMS method details\n",
    "        print(\"3\")\n",
    "        loss_meter.update(loss.item()) #the loss itself is a delta / change\n",
    "        print(\"BPdone\")\n",
    "        with torch.no_grad():\n",
    "            pred_y = odeint(func, true_y0, t)\n",
    "            #loss = torch.mean(torch.abs(pred_y-true_y)**2)\n",
    "            #-------------------------------------------------\n",
    "            predSl = getSlopesTorch(X.numpy(),func)\n",
    "            print(\"predSl2\")\n",
    "            \n",
    "            loss = torch.mean(torch.abs(predSl-trueSl)**2)\n",
    "            print(\"loss2\")\n",
    "            print('Iteration {:04d} | Total Loss {:.6f}'.format(itr, loss.item()))\n",
    "        \n",
    "            if itr % test_freq == 0:\n",
    "                visualize(true_y,'b')\n",
    "                visualize(pred_y,'m')\n",
    "                visualizeSl(trueSl,'b')\n",
    "                visualizeSl(predSl,'m')\n",
    "                \n",
    "        #torch.save(func.state_dict(), C:\\Users\\delis\\Desktop\\DEPython\\modelP)\n",
    "                \n",
    "        end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
